{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "n_set = np.arange(len(y_train))\n",
    "sample = np.random.choice(a=n_set, size=10000)\n",
    "x_subset = x_train[sample]\n",
    "y_subset = y_train[sample]\n",
    "\n",
    "bounds1 = np.array([[0.001, 10]])\n",
    "bounds2 = np.array([[1, 100]])\n",
    "\n",
    "def f(params):\n",
    "    print(\"Learning rate:\", params[0])\n",
    "    print(\"Batch size:\", params[1])\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(15, input_dim = 784, activation='sigmoid'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    sgd=SGD(lr=params[0])\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_subset, y_subset, \n",
    "          batch_size=int(params[1]), epochs=10, verbose=0, validation_data = (x_test, y_test))\n",
    "    return -history.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def expected_improvement(X, X_sample, Y_sample, gpr, xi=0.01):\n",
    "    ''' Computes the EI at points X based on existing samples X_sample and Y_sample using a Gaussian process surrogate model. Args: X: Points at which EI shall be computed (m x d). X_sample: Sample locations (n x d). Y_sample: Sample values (n x 1). gpr: A GaussianProcessRegressor fitted to samples. xi: Exploitation-exploration trade-off parameter. Returns: Expected improvements at points X. '''\n",
    "    mu, sigma = gpr.predict(X, return_std=True)\n",
    "    mu_sample = gpr.predict(X_sample)\n",
    "\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    \n",
    "    # Needed for noise-based model,\n",
    "    # otherwise use np.max(Y_sample).\n",
    "    # See also section 2.4 in [...]\n",
    "    mu_sample_opt = np.max(mu_sample)\n",
    "\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu - mu_sample_opt - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def propose_location(acquisition, X_sample, Y_sample, gpr, bounds, n_restarts=25):\n",
    "    ''' Proposes the next sampling point by optimizing the acquisition function. Args: acquisition: Acquisition function. X_sample: Sample locations (n x d). Y_sample: Sample values (n x 1). gpr: A GaussianProcessRegressor fitted to samples. Returns: Location of the acquisition function maximum. '''\n",
    "    dim = X_sample.shape[1]\n",
    "    min_val = 1\n",
    "    min_x = None\n",
    "    \n",
    "    def min_obj(X):\n",
    "        # Minimization objective is the negative acquisition function\n",
    "        return -acquisition(X.reshape(-1, dim), X_sample, Y_sample, gpr)\n",
    "    \n",
    "    # Find the best optimum by starting from n_restart different random points.\n",
    "    X1 = np.arange(bounds1[:, 0], bounds1[:, 1], 0.01).reshape(-1, 1)\n",
    "    X2 = np.arange(bounds2[:, 0], bounds2[:, 1]).reshape(-1, 1)\n",
    "    X=np.array(np.meshgrid(X1, X2)).T.reshape(-1,2)\n",
    "    n_set = np.arange(X.shape[0])\n",
    "    sample = np.random.choice(n_set, 25)\n",
    "    for x0 in X[sample]:\n",
    "        res = minimize(min_obj, x0=(x0[0], x0[1]), bounds=bounds, method='L-BFGS-B')        \n",
    "        if res.fun < min_val:\n",
    "            min_val = res.fun[0]\n",
    "            min_x = res.x           \n",
    "            \n",
    "    return min_x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern\n",
    "def bayesopt_multi(n_iter, X_init, Y_init, bounds1, bounds2, X1, X2):\n",
    "  # Gaussian process with Mat??rn kernel as surrogate model\n",
    "  m52 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)\n",
    "  gpr = GaussianProcessRegressor(kernel=m52, alpha=0.02**2)\n",
    "  # Initialize samples\n",
    "  X_sample = X_init\n",
    "  Y_sample = Y_init\n",
    "  best_acc = np.zeros(n_iter+1)\n",
    "  best_acc[0] = -1 * Y_init.min()\n",
    "  # Number of iterations\n",
    "\n",
    "  for i in range(n_iter):\n",
    "      print(\"Training model\", i+1)\n",
    "      # Update Gaussian process with existing samples\n",
    "      gpr.fit(X_sample, Y_sample)\n",
    "\n",
    "      # Obtain next sampling point from the acquisition function (expected_improvement)\n",
    "      X_next = propose_location(expected_improvement, X_sample, Y_sample, gpr, ((bounds1[:, 0][0], bounds1[:, 1][0]), (bounds2[:, 0][0], bounds2[:, 1][0])))\n",
    "    \n",
    "      # Obtain next noisy sample from the objective function\n",
    "      Y_next = f(X_next)\n",
    "    \n",
    "      # Add sample to previous samples\n",
    "      X_sample = np.column_stack((X_sample.T, X_next)).T\n",
    "      Y_sample = np.vstack((Y_sample, np.array(Y_next).reshape(-1,1)))\n",
    "      best_acc[i+1] = -1 * Y_sample.min()\n",
    "  return gpr, X_sample, Y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0728 21:38:16.040069 139930299455296 deprecation_wrapper.py:119] From /home/aa710/crowd_nets/jupyterenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0728 21:38:16.104365 139930299455296 deprecation_wrapper.py:119] From /home/aa710/crowd_nets/jupyterenv/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0728 21:38:16.109016 139930299455296 deprecation_wrapper.py:119] From /home/aa710/crowd_nets/jupyterenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0728 21:38:16.134730 139930299455296 deprecation_wrapper.py:119] From /home/aa710/crowd_nets/jupyterenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0728 21:38:16.214665 139930299455296 deprecation_wrapper.py:119] From /home/aa710/crowd_nets/jupyterenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.255\n",
      "Batch size: 5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0728 21:38:16.468064 139930299455296 deprecation.py:323] From /home/aa710/crowd_nets/jupyterenv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0728 21:38:16.579225 139930299455296 deprecation_wrapper.py:119] From /home/aa710/crowd_nets/jupyterenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1.1849999999999998\n",
      "Batch size: 9.0\n",
      "First team...\n",
      "Training model 1\n",
      "Learning rate: [0.05100604]\n",
      "Batch size: [31.00000604]\n",
      "Training model 2\n",
      "Learning rate: [0.821]\n",
      "Batch size: [31.]\n",
      "Training model 3\n",
      "Learning rate: [0.441]\n",
      "Batch size: [1.]\n",
      "Training model 4\n",
      "Learning rate: [1.61100544]\n",
      "Batch size: [2.00000272]\n",
      "Training model 5\n",
      "Learning rate: [0.231]\n",
      "Batch size: [1.]\n",
      "Training model 6\n",
      "Learning rate: [1.831]\n",
      "Batch size: [1.]\n",
      "Training model 7\n",
      "Learning rate: [2.]\n",
      "Batch size: [19.9452416]\n",
      "Training model 8\n",
      "Learning rate: [0.001]\n",
      "Batch size: [14.59529812]\n",
      "Training model 9\n",
      "Learning rate: [0.001]\n",
      "Batch size: [25.19897333]\n",
      "Training model 10\n",
      "Learning rate: [2.]\n",
      "Batch size: [26.43064042]\n",
      "Learning rate: 9.419999999999183\n",
      "Batch size: 184.0\n",
      "Learning rate: 10.80799999999903\n",
      "Batch size: 149.0\n",
      "Second team...\n",
      "Training model 1\n",
      "Learning rate: [6.48]\n",
      "Batch size: [96.]\n",
      "Training model 2\n",
      "Learning rate: [7.98]\n",
      "Batch size: [58.]\n",
      "Training model 3\n",
      "Learning rate: [20.82]\n",
      "Batch size: [114.]\n",
      "Training model 4\n",
      "Learning rate: [17.04]\n",
      "Batch size: [75.]\n",
      "Training model 5\n",
      "Learning rate: [3.1]\n",
      "Batch size: [33.]\n",
      "Training model 6\n",
      "Learning rate: [3.54]\n",
      "Batch size: [165.]\n",
      "Training model 7\n",
      "Learning rate: [22.05]\n",
      "Batch size: [35.]\n",
      "Training model 8\n",
      "Learning rate: [2.11]\n",
      "Batch size: [123.]\n",
      "Training model 9\n",
      "Learning rate: [24.31]\n",
      "Batch size: [58.]\n",
      "Training model 10\n",
      "Learning rate: [23.01]\n",
      "Batch size: [98.]\n"
     ]
    }
   ],
   "source": [
    "acc_ch = np.zeros((3,10))\n",
    "acc_rn = np.zeros((3, 10))\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "  bounds1 = np.array([[0.001, 2]])\n",
    "  bounds2 = np.array([[1, 32]])\n",
    "  X1 = np.arange(bounds1[:, 0], bounds1[:, 1], 0.001).reshape(-1, 1)\n",
    "  X2 = np.arange(bounds2[:, 0], bounds2[:, 1], 1).reshape(-1, 1)\n",
    "  x1=np.array(np.meshgrid(X1, X2)).T.reshape(-1,2)\n",
    "  X_init = x1[np.random.randint(x1.shape[0], size=2)]\n",
    "  y1 = f(X_init[0])\n",
    "  y2 = f(X_init[1])\n",
    "  Y_init = np.array([[y1],[y2]])\n",
    "  print(\"First team...\")\n",
    "  gpr1, x1_sample, y1_sample = bayesopt_multi(10, X_init, Y_init, bounds1, bounds2, X1, X2)\n",
    "\n",
    "  bounds1 = np.array([[2, 25]])\n",
    "  bounds2 = np.array([[32, 200]])\n",
    "  X1 = np.arange(bounds1[:, 0], bounds1[:, 1], 0.001).reshape(-1, 1)\n",
    "  X2 = np.arange(bounds2[:, 0], bounds2[:, 1], 1).reshape(-1, 1)\n",
    "  x2=np.array(np.meshgrid(X1, X2)).T.reshape(-1,2)\n",
    "  X_init = x2[np.random.randint(x2.shape[0], size=2)]\n",
    "  y1 = f(X_init[0])\n",
    "  y2 = f(X_init[1])\n",
    "  Y_init = np.array([[y1],[y2]])\n",
    "  print(\"Second team...\")\n",
    "  gpr2, x2_sample, y2_sample = bayesopt_multi(10, X_init, Y_init, bounds1, bounds2, X1, X2)\n",
    "  \n",
    "  mu1 = gpr1.predict(x1)\n",
    "  mu1[mu1 < -1] = 0\n",
    "  mu2 = gpr2.predict(x2)\n",
    "  mu2[mu2 < -1] = 0\n",
    "\n",
    "  minimum = mu1.min() if mu1.min() <= mu2.min() else mu2.min()\n",
    "\n",
    "  params = x1[np.where(mu1==mu1.min())[0][0]] if mu1.min() <= mu2.min() else x2[np.where(mu2==mu2.min())[0][0]]\n",
    "  \n",
    "  print(\"Third team...\")\n",
    "  print(\"Learning rate:\", params[0])\n",
    "  print(\"Batch size:\", params[1])\n",
    "\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(15, input_dim = 784, activation='sigmoid'))\n",
    "  model.add(Dense(10, activation='softmax'))\n",
    "  sgd=SGD(lr=params[0])\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "  history = model.fit(x_subset, y_subset, \n",
    "          batch_size=int(params[1]), epochs=10, verbose=1, validation_data = (x_test, y_test))\n",
    "  \n",
    "  acc_ch[0,i] = history.history['acc'][-1]\n",
    "  acc_ch[1,i] = history.history['val_acc'][-1]\n",
    "  acc_ch[2,i] = minimum\n",
    "  \n",
    "  choice = np.random.randint(2)+1\n",
    "  params = x1[np.where(mu1==mu1.min())[0][0]] if choice==1 else x2[np.where(mu2==mu2.min())[0][0]]\n",
    "  minimum = mu1.min() if choice==1 else mu2.min()\n",
    "  \n",
    "  model = Sequential()\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(15, input_dim = 784, activation='sigmoid'))\n",
    "  model.add(Dense(10, activation='softmax'))\n",
    "  sgd=SGD(lr=params[0])\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "  history = model.fit(x_subset, y_subset, \n",
    "          batch_size=int(params[1]), epochs=10, verbose=1, validation_data = (x_test, y_test))\n",
    "  \n",
    "  acc_rn[0,i] = history.history['acc'][-1]\n",
    "  acc_rn[1,i] = history.history['val_acc'][-1]\n",
    "  acc_rn[2,i] = minimum\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
